# llm_jepa_v2

1. This repo uses poetry.
2. `train.py` is the main entrypoint

## Run with Google Compute Engine + Docker

This project uses Docker; the `Dockerfile` handles all dependency management and setup.

### Build the Image

From the project root (where the Dockerfile lives):

(For CPU (NOT RECOMMENDED: Compute is too intenstive for CPU, but can be used to test dependencies))

```bash
docker build -f Dockerfile.cpu -t myapp:cpu .
docker run --rm myapp:cpu
```

(For GPU (RECOMMENDED))

```bash
docker build -f Dockerfile.gpu -t myapp:gpu .
docker run --rm --gpus all myapp:gpu
```

Then run

```bash
./submit_job_to_gcp_compute.sh
```

-v "$PWD":/app mounts your current directory so code changes are visible inside the container.
--gpus all exposes all available GPUs to the container (omit for CPU).


### Running on Gadi

We use `conda`.


To activate `conda` we do - `conda activate cw9909`
To run on deepspeed you need to - `module load cuda/12.8.0`
`deepspeed --num_gpus 4 /home/561/cw9909/experiments/llm_jepa_v2/train.py`


To submit a job, we run - `qsub run_train_two_a100.pbs` this will return a runid

To monitor the status of the run we do `qstat runid`
To see the output of a run, a <script_name>.pbs.o<runid> will automatically be generated by Gadi after the run is complete.


To start an interactive session (Useful for testing code before a job submission), run this code - 
```
qsub -I -P oy87 -q dgxa100 \
  -l walltime=04:00:00,ncpus=64,ngpus=4,mem=64GB,jobfs=200GB,storage=gdata/oy87+scratch/oy87
```