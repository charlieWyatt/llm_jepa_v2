# llm_jepa_v2

1. This repo uses poetry.
2. `train.py` is the main entrypoint

## Run with Google Compute Engine + Docker

This project uses Docker; the `Dockerfile` handles all dependency management and setup.

### Build the Image

From the project root (where the Dockerfile lives):

(For CPU (NOT RECOMMENDED: Compute is too intenstive for CPU, but can be used to test dependencies))

```bash
docker build -f Dockerfile.cpu -t myapp:cpu .
docker run --rm myapp:cpu
```

(For GPU (RECOMMENDED))

```bash
docker build -f Dockerfile.gpu -t myapp:gpu .
docker run --rm --gpus all myapp:gpu
```

Then run

```bash
./submit_job_to_gcp_compute.sh
```

-v "$PWD":/app mounts your current directory so code changes are visible inside the container.
--gpus all exposes all available GPUs to the container (omit for CPU).


### Running on Gadi

We use `conda`.


To activate `conda` we do - `conda activate cw9909`

To submit a job, we run - `qsub run_train_two_a100.pbs` this will return a runid

To monitor the status of the run we do `qstat runid`
To see the output of a run, a <script_name>.pbs.o<runid> will automatically be generated by Gadi after the run is complete.